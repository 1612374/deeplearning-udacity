{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_convolutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "4embtkV0pNxM"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 4\n",
        "------------\n",
        "\n",
        "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
        "\n",
        "The goal of this assignment is make the neural network convolutional."
      ]
    },
    {
      "metadata": {
        "id": "q04J1kRpOoNS",
        "colab_type": "code",
        "outputId": "536a1d7a-1b13-4cd2-e5a5-48a3d766bdc6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9144a9f-81dc-4529-81c2-146e6c172d41\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d9144a9f-81dc-4529-81c2-146e6c172d41\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving notMNIST.pickle to notMNIST.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "tm2CQN_Cpwj0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "y3-cj1bpmuxc",
        "outputId": "acb4d4ad-3c75-4920-adce-7a4f1aa15e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L7aHrm6nGDMB"
      },
      "cell_type": "markdown",
      "source": [
        "Reformat into a TensorFlow-friendly shape:\n",
        "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "IRSyYiIIGIzS",
        "outputId": "f4f2d63d-c1e7-479e-8ad7-f1de539a9886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "num_channels = 1 # grayscale\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape(\n",
        "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28, 1) (200000, 10)\n",
            "Validation set (10000, 28, 28, 1) (10000, 10)\n",
            "Test set (10000, 28, 28, 1) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "AgQDIREv02p1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5rhgjmROXu2O"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "IZYv70SvvOan",
        "outputId": "c2aa3b8d-0074-496d-d1c3-f6f94f3c9c90",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "patch_size = 5\n",
        "depth = 16\n",
        "num_hidden = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data.\n",
        "  tf_train_dataset = tf.placeholder(\n",
        "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
        "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
        "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
        "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
        "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
        "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
        "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
        "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
        "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
        "      [num_hidden, num_labels], stddev=0.1))\n",
        "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
        "  \n",
        "  # Model.\n",
        "  def model(data):\n",
        "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
        "    hidden = tf.nn.relu(conv + layer1_biases)\n",
        "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
        "    hidden = tf.nn.relu(conv + layer2_biases)\n",
        "    shape = hidden.get_shape().as_list()\n",
        "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
        "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
        "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
        "  \n",
        "  # Training computation.\n",
        "  logits = model(tf_train_dataset)\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
        "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/quangminh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "noKFb2UovVFR",
        "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_steps = 1001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  for step in range(num_steps):\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "   \n",
        "    if (step % 50 == 0):\n",
        "      print('Minibatch loss at step %d: %f' % (step, l))\n",
        "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
        "      print('Validation accuracy: %.1f%%' % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3.552683\n",
            "Minibatch accuracy: 12.5%\n",
            "Validation accuracy: 10.0%\n",
            "Minibatch loss at step 50: 1.129061\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 53.8%\n",
            "Minibatch loss at step 100: 1.719806\n",
            "Minibatch accuracy: 56.2%\n",
            "Validation accuracy: 72.7%\n",
            "Minibatch loss at step 150: 1.127680\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 200: 1.006077\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 76.2%\n",
            "Minibatch loss at step 250: 0.412526\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 79.8%\n",
            "Minibatch loss at step 300: 0.663652\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 79.5%\n",
            "Minibatch loss at step 350: 0.292112\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 80.7%\n",
            "Minibatch loss at step 400: 0.529483\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 80.7%\n",
            "Minibatch loss at step 450: 0.466049\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 81.4%\n",
            "Minibatch loss at step 500: 0.174193\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 81.5%\n",
            "Minibatch loss at step 550: 0.614626\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 81.2%\n",
            "Minibatch loss at step 600: 0.444284\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.3%\n",
            "Minibatch loss at step 650: 0.394984\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 81.9%\n",
            "Minibatch loss at step 700: 0.938070\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 83.0%\n",
            "Minibatch loss at step 750: 0.761209\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 83.4%\n",
            "Minibatch loss at step 800: 0.451169\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.2%\n",
            "Minibatch loss at step 850: 0.802724\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 83.0%\n",
            "Minibatch loss at step 900: 0.462594\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 83.7%\n",
            "Minibatch loss at step 950: 0.319782\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.6%\n",
            "Minibatch loss at step 1000: 0.075859\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 84.1%\n",
            "Test accuracy: 90.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KedKkn4EutIK"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "fkM0dDy4Okwg",
        "colab_type": "code",
        "outputId": "b2a5ff54-f25b-484e-a4ea-c606a7757e37",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "patch_size = 5 #kernel size\n",
        "image_size = 28\n",
        "depth = 16\n",
        "num_hidden = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    #placeholder\n",
        "    tf_train_dataset = tf.placeholder(dtype = tf.float32, shape = (batch_size, image_size, image_size, num_channels))\n",
        "    tf_train_labels = tf.placeholder(dtype = tf.float32, shape = (batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "    # Weight\n",
        "    layer1_weight = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = 0.1))\n",
        "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "    \n",
        "    layer2_weight = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = 0.1))\n",
        "    layer2_biases = tf.Variable(tf.zeros([depth]))\n",
        "    \n",
        "    layer3_weight = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev = 0.1))\n",
        "    layer3_biases = tf.Variable(tf.zeros([num_hidden]))\n",
        "\n",
        "    layer4_weight = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1))\n",
        "    layer4_biases = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    #compute\n",
        "    def model(data):\n",
        "        conv = tf.nn.conv2d(input = data, filter = layer1_weight, strides = [1,1,1,1], padding = 'SAME')\n",
        "        hidden = tf.nn.relu(conv + layer1_biases)\n",
        "        pooling = tf.nn.max_pool(value = hidden, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "        \n",
        "        conv = tf.nn.conv2d(input = pooling, filter = layer2_weight, strides = [1,1,1,1], padding = 'SAME')\n",
        "        hidden = tf.nn.relu(conv + layer2_biases)\n",
        "        pooling = tf.nn.max_pool(value = hidden, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "        \n",
        "        shape = pooling.get_shape().as_list()\n",
        "        reshape = tf.reshape(pooling, [shape[0], shape[1]*shape[2]*shape[3]])\n",
        "        hidden = tf.matmul(reshape, layer3_weight) + layer3_biases\n",
        "        \n",
        "        return tf.matmul(hidden, layer4_weight) + layer4_biases\n",
        "        \n",
        "        \n",
        "    logits = model(tf_train_dataset)\n",
        "    \n",
        "    loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
        "    \n",
        "    #optimizer\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
        "    \n",
        "    #predict\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
        "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/quangminh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fXnwGMviOkwk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Run session"
      ]
    },
    {
      "metadata": {
        "id": "Y_F4eFRvOkwl",
        "colab_type": "code",
        "outputId": "3fded3b5-1a15-430d-c7a1-f0f1d55ef4ad",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_step = 1001\n",
        "\n",
        "with tf.Session(graph = graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Init variable')\n",
        "    \n",
        "    for step in range(num_step):\n",
        "        offset = (batch_size * step) % (train_dataset.shape[0] - batch_size)\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        \n",
        "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels: batch_labels}\n",
        "        \n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
        "        \n",
        "        if (step % 50 == 0):\n",
        "            print('Minibatch loss at step %d: %f' % (step, l))\n",
        "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
        "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        \n",
        "    \n",
        "    print('Test accuracy: %f' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init variable\n",
            "Minibatch loss at step 0: 2.232879\n",
            "Minibatch accuracy: 18.8%\n",
            "Validation accuracy: 14.2%\n",
            "Minibatch loss at step 50: 0.924045\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 65.0%\n",
            "Minibatch loss at step 100: 1.388810\n",
            "Minibatch accuracy: 62.5%\n",
            "Validation accuracy: 76.2%\n",
            "Minibatch loss at step 150: 0.804022\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 79.7%\n",
            "Minibatch loss at step 200: 1.139033\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 81.0%\n",
            "Minibatch loss at step 250: 0.450202\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 81.4%\n",
            "Minibatch loss at step 300: 0.699687\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.7%\n",
            "Minibatch loss at step 350: 0.394132\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 82.8%\n",
            "Minibatch loss at step 400: 0.666866\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.5%\n",
            "Minibatch loss at step 450: 0.433673\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.3%\n",
            "Minibatch loss at step 500: 0.232778\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 84.0%\n",
            "Minibatch loss at step 550: 0.412488\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 83.7%\n",
            "Minibatch loss at step 600: 0.403104\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 84.4%\n",
            "Minibatch loss at step 650: 0.251811\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 84.4%\n",
            "Minibatch loss at step 700: 1.269316\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 85.3%\n",
            "Minibatch loss at step 750: 0.678335\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 85.1%\n",
            "Minibatch loss at step 800: 0.530979\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 84.8%\n",
            "Minibatch loss at step 850: 0.683744\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 85.3%\n",
            "Minibatch loss at step 900: 0.197080\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 85.4%\n",
            "Minibatch loss at step 950: 0.287057\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 85.5%\n",
            "Minibatch loss at step 1000: 0.186069\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 85.4%\n",
            "Test accuracy: 91.140000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "klf21gpbAgb-"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "\n",
        "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "ap6N0nBe6mQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some notice:\n",
        "+ Depth of conv layer\n",
        "+ Number of step\n"
      ]
    },
    {
      "metadata": {
        "id": "JzEo08n5Okwr",
        "colab_type": "code",
        "outputId": "d58d4ccd-b7f2-46d7-faec-d840c811dcb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "depth = 64\n",
        "depth1 = 64\n",
        "depth2 = 64\n",
        "\n",
        "patch_size = 5\n",
        "\n",
        "num_hidden1 = 1024\n",
        "num_hidden2 = 512\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    \n",
        "    # placeholder\n",
        "    tf_train_dataset = tf.placeholder(dtype = tf.float32, shape = (batch_size, image_size, image_size, num_channels))\n",
        "    tf_train_labels = tf.placeholder(dtype = tf.float32, shape = (batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "    #weight\n",
        "    #3 conv\n",
        "    layer1_weight = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = 0.1))\n",
        "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
        "    \n",
        "    layer2_weight = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth1], stddev = 0.1))\n",
        "    layer2_biases = tf.Variable(tf.zeros([depth1]))\n",
        "    \n",
        "    layer3_weight = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev = 0.1))\n",
        "    layer3_biases = tf.Variable(tf.zeros([depth2]))\n",
        "    \n",
        "    # 2 fc\n",
        "    layer4_weight = tf.Variable(tf.truncated_normal([16*depth2, num_hidden1], stddev = 0.1))\n",
        "    layer4_biases = tf.Variable(tf.zeros(num_hidden1))\n",
        "    \n",
        "    layer5_weight = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev = 0.1))\n",
        "    layer5_biases = tf.Variable(tf.zeros(num_hidden2))\n",
        "    \n",
        "    # to output\n",
        "    layer6_weight = tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev = 0.1))\n",
        "    layer6_biases = tf.Variable(tf.zeros(num_labels))\n",
        "    \n",
        "    #compute\n",
        "    def model(data):\n",
        "        #conv1\n",
        "        conv = tf.nn.conv2d(input = data, filter = layer1_weight, strides = [1,1,1,1], padding = 'SAME')\n",
        "        hidden = tf.nn.relu(conv + layer1_biases)\n",
        "        pooling = tf.nn.max_pool(value = hidden, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "        \n",
        "        #conv2\n",
        "        conv = tf.nn.conv2d(input = pooling, filter = layer2_weight, strides = [1,1,1,1], padding = 'SAME')\n",
        "        hidden = tf.nn.relu(conv + layer2_biases)\n",
        "        pooling = tf.nn.max_pool(value = hidden, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "        \n",
        "        #conv3\n",
        "        conv = tf.nn.conv2d(input = pooling, filter = layer3_weight, strides = [1,1,1,1], padding = 'SAME')\n",
        "        hidden = tf.nn.relu(conv + layer3_biases)\n",
        "        pooling = tf.nn.max_pool(value = hidden, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "        \n",
        "        #fc1\n",
        "        shape = pooling.get_shape().as_list()\n",
        "        reshape = tf.reshape(pooling, [shape[0], shape[1]*shape[2]*shape[3]])\n",
        "        hidden1 = tf.nn.relu(tf.matmul(reshape, layer4_weight) + layer4_biases)\n",
        "        \n",
        "        #dropout1\n",
        "        hidden1 = tf.nn.dropout(hidden1, keep_prob = 0.5)\n",
        "        \n",
        "        \n",
        "        #fc2 \n",
        "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer5_weight) + layer5_biases)\n",
        "        \n",
        "        #dropout2\n",
        "        hidden2 = tf.nn.dropout(hidden2, keep_prob = 0.5)\n",
        "        \n",
        "        #output\n",
        "        return tf.matmul(hidden2, layer6_weight) + layer6_biases\n",
        "    \n",
        "    logits = model(tf_train_dataset)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = tf_train_labels))\n",
        "    \n",
        "    \n",
        "    #optimizer\n",
        "    optimizer =tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "    #global_step = tf.Variable(0)\n",
        "    #starter_learning_rate = 0.5\n",
        "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,500,0.96)\n",
        "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "    \n",
        "    #prediction\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
        "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-6-0a6244872d7d>:66: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wSjXK0fqOkwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Run session"
      ]
    },
    {
      "metadata": {
        "id": "T-JwCfQbOkwv",
        "colab_type": "code",
        "outputId": "b3ef5693-1a54-4039-fa6f-20619d277d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1122
        }
      },
      "cell_type": "code",
      "source": [
        "num_step = 100001\n",
        "\n",
        "with tf.Session(graph = graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Init variable')\n",
        "    \n",
        "    for step in range(num_step):\n",
        "        offset = (batch_size * step) % (train_dataset.shape[0] - batch_size)\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        \n",
        "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels: batch_labels}\n",
        "        \n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
        "        \n",
        "        if (step % 5000 == 0):\n",
        "            print('Minibatch loss at step %d: %f' % (step, l))\n",
        "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
        "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        \n",
        "    \n",
        "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init variable\n",
            "Minibatch loss at step 0: 37.813530\n",
            "Minibatch accuracy: 18.8%\n",
            "Validation accuracy: 9.4%\n",
            "Minibatch loss at step 5000: 0.247193\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 82.6%\n",
            "Minibatch loss at step 10000: 0.033372\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 86.5%\n",
            "Minibatch loss at step 15000: 0.583697\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 88.0%\n",
            "Minibatch loss at step 20000: 0.641897\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 88.6%\n",
            "Minibatch loss at step 25000: 0.292957\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 89.3%\n",
            "Minibatch loss at step 30000: 0.329637\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 89.4%\n",
            "Minibatch loss at step 35000: 0.196437\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.0%\n",
            "Minibatch loss at step 40000: 0.472378\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 45000: 0.245666\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 50000: 0.060818\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 90.7%\n",
            "Minibatch loss at step 55000: 0.226319\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 60000: 0.335831\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 65000: 0.276860\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 91.3%\n",
            "Minibatch loss at step 70000: 0.442353\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 91.6%\n",
            "Minibatch loss at step 75000: 0.466153\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 91.3%\n",
            "Minibatch loss at step 80000: 0.457852\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 91.7%\n",
            "Minibatch loss at step 85000: 0.001096\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.7%\n",
            "Minibatch loss at step 90000: 0.133027\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 91.7%\n",
            "Minibatch loss at step 95000: 0.201945\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 91.7%\n",
            "Minibatch loss at step 100000: 0.433263\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 91.9%\n",
            "Test accuracy: 96.3%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}